{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognize named entities on Twitter with LSTMs\n",
    "\n",
    "For example, we want to extract persons' and organizations' names from the text. Than for the input text:\n",
    "\n",
    "    Ian Goodfellow works for Google Brain\n",
    "\n",
    "a NER model needs to provide the following sequence of tags:\n",
    "\n",
    "    B-PER I-PER    O     O   B-ORG  I-ORG\n",
    "\n",
    "Where *B-* and *I-* prefixes stand for the beginning and inside of the entity, while *O* stands for out of tag or no tag. Markup with the prefix scheme is called *BIO markup*. This markup is introduced for distinguishing of consequent entities with similar types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from evaluation import precision_recall_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    \n",
    "    tweet_tokens = []\n",
    "    tweet_tags = []\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            if tweet_tokens:\n",
    "                tokens.append(tweet_tokens)\n",
    "                tags.append(tweet_tags)\n",
    "            tweet_tokens = []\n",
    "            tweet_tags = []\n",
    "        else:\n",
    "            token, tag = line.split()\n",
    "            \n",
    "            if token.startswith(\"http://\") or token.startswith(\"https://\"):\n",
    "                token = \"<URL>\"\n",
    "                \n",
    "            if token.startswith(\"@\"):\n",
    "                token = \"<USR>\"\n",
    "            \n",
    "            tweet_tokens.append(token)\n",
    "            tweet_tags.append(tag)\n",
    "            \n",
    "    return tokens, tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can load three separate parts of the dataset:\n",
    " - *train* data for training the model;\n",
    " - *validation* data for evaluation and hyperparameters tuning;\n",
    " - *test* data for final evaluation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens, train_tags = read_data('data/train.txt')\n",
    "validation_tokens, validation_tags = read_data('data/validation.txt')\n",
    "test_tokens, test_tags = read_data('data/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5795 724 724\n"
     ]
    }
   ],
   "source": [
    "print(len(train_tokens), len(validation_tokens), len(test_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should always understand what kind of data you deal with. For this purpose, you can print the data running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT\t\tO\n",
      "<USR>\t\tO\n",
      ":\t\tO\n",
      "Online\t\tO\n",
      "ticket\t\tO\n",
      "sales\t\tO\n",
      "for\t\tO\n",
      "Ghostland\t\tB-musicartist\n",
      "Observatory\t\tI-musicartist\n",
      "extended\t\tO\n",
      "until\t\tO\n",
      "6\t\tO\n",
      "PM\t\tO\n",
      "EST\t\tO\n",
      "due\t\tO\n",
      "to\t\tO\n",
      "high\t\tO\n",
      "demand\t\tO\n",
      ".\t\tO\n",
      "Get\t\tO\n",
      "them\t\tO\n",
      "before\t\tO\n",
      "they\t\tO\n",
      "sell\t\tO\n",
      "out\t\tO\n",
      "...\t\tO\n",
      "\n",
      "Apple\t\tB-product\n",
      "MacBook\t\tI-product\n",
      "Pro\t\tI-product\n",
      "A1278\t\tI-product\n",
      "13.3\t\tI-product\n",
      "\"\t\tI-product\n",
      "Laptop\t\tI-product\n",
      "-\t\tI-product\n",
      "MD101LL/A\t\tI-product\n",
      "(\t\tO\n",
      "June\t\tO\n",
      ",\t\tO\n",
      "2012\t\tO\n",
      ")\t\tO\n",
      "-\t\tO\n",
      "Full\t\tO\n",
      "read\t\tO\n",
      "by\t\tO\n",
      "eBay\t\tB-company\n",
      "<URL>\t\tO\n",
      "<URL>\t\tO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    for token, tag in zip(train_tokens[i], train_tags[i]):\n",
    "        print('%s\\t\\t%s' % (token, tag))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(tokens_or_tags, special_tokens):\n",
    "    \"\"\"\n",
    "        tokens_or_tags: a list of lists of tokens or tags\n",
    "        special_tokens: some special tokens\n",
    "    \"\"\"    \n",
    "    i = 0\n",
    "    vocab = set([t for ts in tokens_or_tags for t in ts])\n",
    "    vocab_size = len(vocab)+len(special_tokens)\n",
    "    \n",
    "    tok2idx = defaultdict(lambda: 0)    \n",
    "    idx2tok = {}\n",
    "    \n",
    "    for t in special_tokens:\n",
    "        tok2idx[t] = i\n",
    "        idx2tok[i] = t\n",
    "        i +=1\n",
    "                \n",
    "    for t_list in tokens_or_tags:\n",
    "        \n",
    "        for w in t_list:\n",
    "            \n",
    "            if w not in tok2idx:\n",
    "                tok2idx[w] = i\n",
    "                idx2tok[i] = w\n",
    "                i+=1\n",
    "    \n",
    "    return tok2idx, idx2tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing the function *build_dict* you can make dictionaries for tokens and tags. Special tokens in our case will be:\n",
    " - `<UNK>` token for out of vocabulary tokens;\n",
    " - `<PAD>` token for padding sentence to the same length when we create batches of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = ['<UNK>', '<PAD>']\n",
    "special_tags = ['O']\n",
    "\n",
    "# Create dictionaries\n",
    "token2idx, idx2token = build_dict(train_tokens + validation_tokens, special_tokens)\n",
    "tag2idx, idx2tag = build_dict(train_tags + validation_tags, special_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-musicartist',\n",
       " 2: 'I-musicartist',\n",
       " 3: 'B-product',\n",
       " 4: 'I-product',\n",
       " 5: 'B-company',\n",
       " 6: 'B-person',\n",
       " 7: 'B-other',\n",
       " 8: 'I-other',\n",
       " 9: 'B-facility',\n",
       " 10: 'I-facility',\n",
       " 11: 'B-sportsteam',\n",
       " 12: 'B-geo-loc',\n",
       " 13: 'I-geo-loc',\n",
       " 14: 'I-company',\n",
       " 15: 'I-person',\n",
       " 16: 'B-movie',\n",
       " 17: 'I-movie',\n",
       " 18: 'B-tvshow',\n",
       " 19: 'I-tvshow',\n",
       " 20: 'I-sportsteam'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words2idxs(tokens_list):\n",
    "    return [token2idx[word] for word in tokens_list]\n",
    "\n",
    "def tags2idxs(tags_list):\n",
    "    return [tag2idx[tag] for tag in tags_list]\n",
    "\n",
    "def idxs2words(idxs):\n",
    "    return [idx2token[idx] for idx in idxs]\n",
    "\n",
    "def idxs2tags(idxs):\n",
    "    return [idx2tag[idx] for idx in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches_generator(batch_size, tokens, tags,\n",
    "                      shuffle=True, allow_smaller_last_batch=True):\n",
    "    \"\"\"Generates padded batches of tokens and tags.\"\"\"\n",
    "    \n",
    "    n_samples = len(tokens)\n",
    "    if shuffle:\n",
    "        order = np.random.permutation(n_samples)\n",
    "    else:\n",
    "        order = np.arange(n_samples)\n",
    "\n",
    "    n_batches = n_samples // batch_size\n",
    "    if allow_smaller_last_batch and n_samples % batch_size:\n",
    "        n_batches += 1\n",
    "\n",
    "    for k in range(n_batches):\n",
    "        \n",
    "        batch_start = k * batch_size\n",
    "        batch_end = min((k + 1) * batch_size, n_samples)\n",
    "        current_batch_size = batch_end - batch_start\n",
    "        \n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        max_len_token = 0\n",
    "        \n",
    "        for idx in order[batch_start: batch_end]:\n",
    "            x_list.append(words2idxs(tokens[idx]))\n",
    "            y_list.append(tags2idxs(tags[idx]))\n",
    "            max_len_token = max(max_len_token, len(tags[idx]))\n",
    "            \n",
    "        # Fill in the data into numpy nd-arrays filled with padding indices.\n",
    "        x = np.ones([current_batch_size, max_len_token], dtype=np.int32) * token2idx['<PAD>']\n",
    "        y = np.ones([current_batch_size, max_len_token], dtype=np.int32) * tag2idx['O']\n",
    "        lengths = np.zeros(current_batch_size, dtype=np.int32)\n",
    "        \n",
    "        for n in range(current_batch_size):\n",
    "            utt_len = len(x_list[n])\n",
    "            x[n, :utt_len] = x_list[n]\n",
    "            lengths[n] = utt_len\n",
    "            y[n, :utt_len] = y_list[n]\n",
    "        \n",
    "        yield x, y, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - *input_batch* — sequences of words (the shape equals to [batch_size, sequence_len]);\n",
    " - *ground_truth_tags* — sequences of tags (the shape equals to [batch_size, sequence_len]);\n",
    " - *lengths* — lengths of not padded sequences (the shape equals to [batch_size]);\n",
    " - *dropout_ph* — dropout keep probability; this placeholder has a predefined value 1;\n",
    " - *learning_rate_ph* — learning rate; we need this placeholder because we want to change the value during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMModel():\n",
    "    \n",
    "    def _declare_placeholders(self):\n",
    "        \"\"\"Specifies placeholders for the model.\"\"\"\n",
    "\n",
    "        # Placeholders for input and ground truth output.\n",
    "        self.input_batch = tf.placeholder(dtype=tf.int32, shape=[None, None], name='input_batch') \n",
    "        self.ground_truth_tags = tf.placeholder(dtype=tf.int32, shape = [None,None], name = \"gt_tags\")\n",
    "\n",
    "        # Placeholder for lengths of the sequences.\n",
    "        self.lengths = tf.placeholder(dtype=tf.int32, shape=[None], name='lengths') \n",
    "\n",
    "        # Placeholder for a dropout keep probability. If we don't feed\n",
    "        # a value for this placeholder, it will be equal to 1.0.\n",
    "        self.dropout_ph = tf.placeholder_with_default(tf.cast(1.0, tf.float32), shape=[])\n",
    "\n",
    "        # Placeholder for a learning rate (tf.float32).\n",
    "        self.learning_rate_ph = tf.placeholder(dtype=tf.float32,shape=[])\n",
    "        \n",
    "    def _build_layers(self, vocabulary_size, embedding_dim, n_hidden_rnn, n_tags):\n",
    "        \"\"\"Specifies bi-LSTM architecture and computes logits for inputs.\"\"\"\n",
    "\n",
    "        initial_embedding_matrix = np.random.randn(vocabulary_size, embedding_dim) / np.sqrt(embedding_dim)\n",
    "        embedding_matrix_variable = tf.Variable(initial_embedding_matrix, dtype=tf.float32)\n",
    "\n",
    "        forward_cell =  tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.BasicLSTMCell(n_hidden_rnn),output_keep_prob = self.dropout_ph, state_keep_prob = self.dropout_ph, input_keep_prob = self.dropout_ph)\n",
    "        backward_cell =  tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.BasicLSTMCell(n_hidden_rnn),output_keep_prob = self.dropout_ph, state_keep_prob = self.dropout_ph, input_keep_prob = self.dropout_ph)\n",
    "\n",
    "        # Look up embeddings for self.input_batch (tf.nn.embedding_lookup).\n",
    "        # Shape: [batch_size, sequence_len, embedding_dim].\n",
    "        embeddings =  tf.nn.embedding_lookup(embedding_matrix_variable, self.input_batch)\n",
    "\n",
    "        # Pass them through Bidirectional Dynamic RNN (tf.nn.bidirectional_dynamic_rnn).\n",
    "        # Shape: [batch_size, sequence_len, 2 * n_hidden_rnn]. \n",
    "        # Also don't forget to initialize sequence_length as self.lengths and dtype as tf.float32.\n",
    "        (rnn_output_fw, rnn_output_bw), _ =  tf.nn.bidirectional_dynamic_rnn(forward_cell, backward_cell, embeddings, self.lengths, dtype=tf.float32)\n",
    "        rnn_output = tf.concat([rnn_output_fw, rnn_output_bw], axis=2)\n",
    "\n",
    "        # Dense layer on top.\n",
    "        # Shape: [batch_size, sequence_len, n_tags].   \n",
    "        self.logits = tf.layers.dense(rnn_output, n_tags, activation=None)\n",
    "        \n",
    "    def _compute_predictions(self):\n",
    "        \"\"\"Transforms logits to probabilities and finds the most probable tags.\"\"\"\n",
    "\n",
    "        # Create softmax (tf.nn.softmax) function\n",
    "        softmax_output = tf.nn.softmax(self.logits)\n",
    "\n",
    "        # Use argmax (tf.argmax) to get the most probable tags\n",
    "        # Don't forget to set axis=-1\n",
    "        # otherwise argmax will be calculated in a wrong way\n",
    "        self.predictions = tf.argmax(softmax_output,axis=-1)\n",
    "\n",
    "        return\n",
    "    \n",
    "    def _compute_loss(self, n_tags, PAD_index):\n",
    "        \"\"\"Computes masked cross-entopy loss with logits.\"\"\"\n",
    "\n",
    "        # Create cross entropy function function (tf.nn.softmax_cross_entropy_with_logits_v2)\n",
    "        ground_truth_tags_one_hot = tf.one_hot(self.ground_truth_tags, n_tags)\n",
    "        loss_tensor =  tf.nn.softmax_cross_entropy_with_logits_v2(ground_truth_tags_one_hot,self.logits)\n",
    "\n",
    "        mask = tf.cast(tf.not_equal(self.input_batch, PAD_index), tf.float32)\n",
    "\n",
    "        self.loss =  tf.reduce_mean(tf.multiply(mask,loss_tensor))\n",
    "    \n",
    "    \n",
    "    def _perform_optimization(self):\n",
    "        \"\"\"Specifies the optimizer and train_op for the model.\"\"\"\n",
    "\n",
    "        # Create an optimizer (tf.train.AdamOptimizer)\n",
    "        self.optimizer =  tf.train.AdamOptimizer(learning_rate=self.learning_rate_ph)\n",
    "        self.grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
    "\n",
    "        clip_norm = tf.cast(1.0, tf.float32)\n",
    "        self.grads_and_vars =  [(tf.clip_by_norm(gv[0],clip_norm), gv[1]) for gv in self.grads_and_vars]\n",
    "\n",
    "        self.train_op = self.optimizer.apply_gradients(self.grads_and_vars)\n",
    "        \n",
    "        \n",
    "    def __init__(self, vocabulary_size, n_tags, embedding_dim, n_hidden_rnn, PAD_index):\n",
    "        self._declare_placeholders()\n",
    "        self._build_layers(vocabulary_size, embedding_dim, n_hidden_rnn, n_tags)\n",
    "        self._compute_predictions()\n",
    "        self._compute_loss(n_tags, PAD_index)\n",
    "        self._perform_optimization()\n",
    "        \n",
    "    def train_on_batch(self, session, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability):\n",
    "    \n",
    "        feed_dict = {self.input_batch: x_batch,\n",
    "                     self.ground_truth_tags: y_batch,\n",
    "                     self.learning_rate_ph: learning_rate,\n",
    "                     self.dropout_ph: dropout_keep_probability,\n",
    "                     self.lengths: lengths}\n",
    "\n",
    "        session.run(self.train_op, feed_dict=feed_dict)\n",
    "        \n",
    "        \n",
    "    def predict_for_batch(self, session, x_batch, lengths):\n",
    "\n",
    "        feed_dict = {self.input_batch : x_batch,\n",
    "                     self.lengths : lengths, \n",
    "                     self.dropout_ph : 1.0}\n",
    "\n",
    "\n",
    "        predictions = session.run(self.predictions,feed_dict)\n",
    "\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tags(model, session, token_idxs_batch, lengths):\n",
    "    \"\"\"Performs predictions and transforms indices to tokens and tags.\"\"\"\n",
    "    \n",
    "    tag_idxs_batch = model.predict_for_batch(session, token_idxs_batch, lengths)\n",
    "    \n",
    "    tags_batch, tokens_batch = [], []\n",
    "    for tag_idxs, token_idxs in zip(tag_idxs_batch, token_idxs_batch):\n",
    "        tags, tokens = [], []\n",
    "        for tag_idx, token_idx in zip(tag_idxs, token_idxs):\n",
    "            \n",
    "            tags.append(idx2tag[tag_idx])\n",
    "            tokens.append(idx2token[token_idx])\n",
    "            \n",
    "        tags_batch.append(tags)\n",
    "        tokens_batch.append(tokens)\n",
    "    return tags_batch, tokens_batch\n",
    "    \n",
    "    \n",
    "def eval_conll(model, session, tokens, tags, short_report=True):\n",
    "    \"\"\"Computes NER quality measures using CONLL shared task script.\"\"\"\n",
    "    \n",
    "    y_true, y_pred = [], []\n",
    "    for x_batch, y_batch, lengths in batches_generator(1, tokens, tags):\n",
    "        \n",
    "        tags_batch, tokens_batch = predict_tags(model, session, x_batch, lengths)\n",
    "        if len(x_batch[0]) != len(tags_batch[0]):\n",
    "            raise Exception(\"Incorrect length of prediction for the input, \"\n",
    "                            \"expected length: %i, got: %i\" % (len(x_batch[0]), len(tags_batch[0])))\n",
    "        predicted_tags = []\n",
    "        ground_truth_tags = []\n",
    "        for gt_tag_idx, pred_tag, token in zip(y_batch[0], tags_batch[0], tokens_batch[0]): \n",
    "            if token != '<PAD>':\n",
    "                ground_truth_tags.append(idx2tag[gt_tag_idx])\n",
    "                predicted_tags.append(pred_tag)\n",
    "\n",
    "        # We extend every prediction and ground truth sequence with 'O' tag\n",
    "        # to indicate a possible end of entity.\n",
    "        y_true.extend(ground_truth_tags + ['O'])\n",
    "        y_pred.extend(predicted_tags + ['O'])\n",
    "        \n",
    "    results = precision_recall_f1(y_true, y_pred, print_results=True, short_report=short_report)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run your experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create *BiLSTMModel* model with the following parameters:\n",
    " - *vocabulary_size* — number of tokens;\n",
    " - *n_tags* — number of tags;\n",
    " - *embedding_dim* — dimension of embeddings, recommended value: 200;\n",
    " - *n_hidden_rnn* — size of hidden layers for RNN, recommended value: 200;\n",
    " - *PAD_index* — an index of the padding token (`<PAD>`).\n",
    "\n",
    "Set hyperparameters. You might want to start with the following recommended values:\n",
    "- *batch_size*: 32;\n",
    "- 4 epochs;\n",
    "- starting value of *learning_rate*: 0.005\n",
    "- *learning_rate_decay*: a square root of 2;\n",
    "- *dropout_keep_probability*: try several values: 0.1, 0.5, 0.9.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-32e9c771b0e1>:26: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-11-32e9c771b0e1>:36: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From <ipython-input-11-32e9c771b0e1>:41: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = BiLSTMModel(vocabulary_size=len(token2idx), n_tags=len(tag2idx), embedding_dim=200, n_hidden_rnn=200, PAD_index=token2idx['<PAD>'])  \n",
    "\n",
    "\n",
    "######### YOUR CODE HERE #############\n",
    "\n",
    "batch_size = 32 \n",
    "n_epochs = 10 \n",
    "learning_rate = 0.005 \n",
    "learning_rate_decay = np.sqrt(2) \n",
    "dropout_keep_probability = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to run the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training... \n",
      "\n",
      "-------------------- Epoch 1 of 10 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 75579 phrases; correct: 191.\n",
      "\n",
      "precision:  0.25%; recall:  4.25%; F1:  0.48\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 9257 phrases; correct: 28.\n",
      "\n",
      "precision:  0.30%; recall:  5.21%; F1:  0.57\n",
      "\n",
      "-------------------- Epoch 2 of 10 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 2687 phrases; correct: 477.\n",
      "\n",
      "precision:  17.75%; recall:  10.63%; F1:  13.29\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 203 phrases; correct: 55.\n",
      "\n",
      "precision:  27.09%; recall:  10.24%; F1:  14.86\n",
      "\n",
      "-------------------- Epoch 3 of 10 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 4418 phrases; correct: 2385.\n",
      "\n",
      "precision:  53.98%; recall:  53.13%; F1:  53.55\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 327 phrases; correct: 138.\n",
      "\n",
      "precision:  42.20%; recall:  25.70%; F1:  31.94\n",
      "\n",
      "-------------------- Epoch 4 of 10 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 4695 phrases; correct: 3354.\n",
      "\n",
      "precision:  71.44%; recall:  74.72%; F1:  73.04\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 399 phrases; correct: 176.\n",
      "\n",
      "precision:  44.11%; recall:  32.77%; F1:  37.61\n",
      "\n",
      "-------------------- Epoch 5 of 10 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 4702 phrases; correct: 3866.\n",
      "\n",
      "precision:  82.22%; recall:  86.12%; F1:  84.13\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 402 phrases; correct: 186.\n",
      "\n",
      "precision:  46.27%; recall:  34.64%; F1:  39.62\n",
      "\n",
      "-------------------- Epoch 6 of 10 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 4607 phrases; correct: 4180.\n",
      "\n",
      "precision:  90.73%; recall:  93.12%; F1:  91.91\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 433 phrases; correct: 194.\n",
      "\n",
      "precision:  44.80%; recall:  36.13%; F1:  40.00\n",
      "\n",
      "-------------------- Epoch 7 of 10 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 4589 phrases; correct: 4348.\n",
      "\n",
      "precision:  94.75%; recall:  96.86%; F1:  95.79\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 495 phrases; correct: 205.\n",
      "\n",
      "precision:  41.41%; recall:  38.18%; F1:  39.73\n",
      "\n",
      "-------------------- Epoch 8 of 10 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 4547 phrases; correct: 4385.\n",
      "\n",
      "precision:  96.44%; recall:  97.68%; F1:  97.06\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 448 phrases; correct: 199.\n",
      "\n",
      "precision:  44.42%; recall:  37.06%; F1:  40.41\n",
      "\n",
      "-------------------- Epoch 9 of 10 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 4545 phrases; correct: 4411.\n",
      "\n",
      "precision:  97.05%; recall:  98.26%; F1:  97.65\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 435 phrases; correct: 194.\n",
      "\n",
      "precision:  44.60%; recall:  36.13%; F1:  39.92\n",
      "\n",
      "-------------------- Epoch 10 of 10 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 4533 phrases; correct: 4427.\n",
      "\n",
      "precision:  97.66%; recall:  98.62%; F1:  98.14\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 477 phrases; correct: 197.\n",
      "\n",
      "precision:  41.30%; recall:  36.69%; F1:  38.86\n",
      "\n",
      "...training finished.\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print('Start training... \\n')\n",
    "for epoch in range(n_epochs):\n",
    "    # For each epoch evaluate the model on train and validation data\n",
    "    print('-' * 20 + ' Epoch {} '.format(epoch+1) + 'of {} '.format(n_epochs) + '-' * 20)\n",
    "    print('Train data evaluation:')\n",
    "    eval_conll(model, sess, train_tokens, train_tags, short_report=True)\n",
    "    print('Validation data evaluation:')\n",
    "    eval_conll(model, sess, validation_tokens, validation_tags, short_report=True)\n",
    "    \n",
    "    # Train the model\n",
    "    for x_batch, y_batch, lengths in batches_generator(batch_size, train_tokens, train_tags):\n",
    "        model.train_on_batch(sess, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability)\n",
    "        \n",
    "    # Decaying the learning rate\n",
    "    learning_rate = learning_rate / learning_rate_decay\n",
    "    \n",
    "print('...training finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Full evaluation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Train set quality: --------------------\n",
      "processed 105778 tokens with 4489 phrases; found: 4523 phrases; correct: 4430.\n",
      "\n",
      "precision:  97.94%; recall:  98.69%; F1:  98.31\n",
      "\n",
      "\t     company: precision:   97.97%; recall:   97.51%; F1:   97.74; predicted:   640\n",
      "\n",
      "\t    facility: precision:   95.96%; recall:   98.41%; F1:   97.17; predicted:   322\n",
      "\n",
      "\t     geo-loc: precision:   98.90%; recall:   99.50%; F1:   99.20; predicted:  1002\n",
      "\n",
      "\t       movie: precision:   89.19%; recall:   97.06%; F1:   92.96; predicted:    74\n",
      "\n",
      "\t musicartist: precision:   98.71%; recall:   99.14%; F1:   98.92; predicted:   233\n",
      "\n",
      "\t       other: precision:   97.02%; recall:   98.81%; F1:   97.91; predicted:   771\n",
      "\n",
      "\t      person: precision:   99.44%; recall:   99.77%; F1:   99.61; predicted:   889\n",
      "\n",
      "\t     product: precision:   99.37%; recall:   99.06%; F1:   99.21; predicted:   317\n",
      "\n",
      "\t  sportsteam: precision:   97.70%; recall:   97.70%; F1:   97.70; predicted:   217\n",
      "\n",
      "\t      tvshow: precision:   82.76%; recall:   82.76%; F1:   82.76; predicted:    58\n",
      "\n",
      "-------------------- Validation set quality: --------------------\n",
      "processed 12836 tokens with 537 phrases; found: 415 phrases; correct: 193.\n",
      "\n",
      "precision:  46.51%; recall:  35.94%; F1:  40.55\n",
      "\n",
      "\t     company: precision:   62.37%; recall:   55.77%; F1:   58.88; predicted:    93\n",
      "\n",
      "\t    facility: precision:   35.48%; recall:   32.35%; F1:   33.85; predicted:    31\n",
      "\n",
      "\t     geo-loc: precision:   67.47%; recall:   49.56%; F1:   57.14; predicted:    83\n",
      "\n",
      "\t       movie: precision:   12.50%; recall:   14.29%; F1:   13.33; predicted:     8\n",
      "\n",
      "\t musicartist: precision:   16.67%; recall:   10.71%; F1:   13.04; predicted:    18\n",
      "\n",
      "\t       other: precision:   36.00%; recall:   33.33%; F1:   34.62; predicted:    75\n",
      "\n",
      "\t      person: precision:   48.21%; recall:   24.11%; F1:   32.14; predicted:    56\n",
      "\n",
      "\t     product: precision:   17.86%; recall:   14.71%; F1:   16.13; predicted:    28\n",
      "\n",
      "\t  sportsteam: precision:   29.41%; recall:   25.00%; F1:   27.03; predicted:    17\n",
      "\n",
      "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     6\n",
      "\n",
      "-------------------- Test set quality: --------------------\n",
      "processed 13258 tokens with 604 phrases; found: 461 phrases; correct: 226.\n",
      "\n",
      "precision:  49.02%; recall:  37.42%; F1:  42.44\n",
      "\n",
      "\t     company: precision:   59.02%; recall:   42.86%; F1:   49.66; predicted:    61\n",
      "\n",
      "\t    facility: precision:   61.29%; recall:   40.43%; F1:   48.72; predicted:    31\n",
      "\n",
      "\t     geo-loc: precision:   78.70%; recall:   51.52%; F1:   62.27; predicted:   108\n",
      "\n",
      "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     5\n",
      "\n",
      "\t musicartist: precision:    9.09%; recall:    3.70%; F1:    5.26; predicted:    11\n",
      "\n",
      "\t       other: precision:   27.42%; recall:   33.01%; F1:   29.96; predicted:   124\n",
      "\n",
      "\t      person: precision:   64.52%; recall:   38.46%; F1:   48.19; predicted:    62\n",
      "\n",
      "\t     product: precision:   12.12%; recall:   14.29%; F1:   13.11; predicted:    33\n",
      "\n",
      "\t  sportsteam: precision:   31.82%; recall:   22.58%; F1:   26.42; predicted:    22\n",
      "\n",
      "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('-' * 20 + ' Train set quality: ' + '-' * 20)\n",
    "train_results = eval_conll(model, sess, train_tokens, train_tags, short_report=False)\n",
    "\n",
    "print('-' * 20 + ' Validation set quality: ' + '-' * 20)\n",
    "validation_results = eval_conll(model, sess, validation_tokens, validation_tags, short_report=False)\n",
    "\n",
    "print('-' * 20 + ' Test set quality: ' + '-' * 20)\n",
    "test_results = eval_conll(model, sess, test_tokens, test_tags, short_report=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_gen = batches_generator(5,train_tokens,train_tags, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens,tags,lens = next(batch_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tags, tokens = predict_tags(model,sess,tokens,lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence:\n",
      "RT <USR> : The only school I would willing go to tomorrow <URL> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Original:\n",
      "O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "\n",
      "Predicted:\n",
      "O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "\n",
      "Sentence:\n",
      "&lt; 3 <USR> always brightens up my day . you should follow her and listen to her wonderful music . &lt; 3 <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Original:\n",
      "O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "\n",
      "Predicted:\n",
      "O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "\n",
      "Sentence:\n",
      "Friday got me like #FridayFeeling <URL> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Original:\n",
      "O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "\n",
      "Predicted:\n",
      "O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "\n",
      "Sentence:\n",
      ". <USR> says ALL customers along George Dieter/Rex Baxter now have water . A water main broke Sat . cutting off services to 524 . <USR>\n",
      "\n",
      "Original:\n",
      "O O O O O O B-geo-loc B-geo-loc I-geo-loc O O O O O O O O O O O O O O O O O\n",
      "\n",
      "Predicted:\n",
      "O O O O O O B-geo-loc B-geo-loc I-geo-loc O O O O O O O O O O O O O O O O O\n",
      "\n",
      "Sentence:\n",
      "UCLA Health System data breach affects 4.5 million patients <URL> … <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Original:\n",
      "B-other I-other I-other O O O O O O O O O O O O O O O O O O O O O O O\n",
      "\n",
      "Predicted:\n",
      "B-other I-other I-other O O O O O O O O O O O O O O O O O O O O O O O\n"
     ]
    }
   ],
   "source": [
    "for i,t in enumerate(tokens):\n",
    "    print(\"\\nSentence:\")\n",
    "    print(\" \".join(t))\n",
    "    print(\"\\nOriginal:\")\n",
    "    print(\" \".join(idxs2tags(tags[i])))\n",
    "    print(\"\\nPredicted:\")\n",
    "    print(\" \".join(predicted_tags[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
